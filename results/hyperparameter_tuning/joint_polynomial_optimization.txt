JOINT HYPERPARAMETER AND ARCHITECTURE OPTIMIZATION - POLYNOMIAL (PyTorch)
======================================================================

Methodology: Two-phase per-algorithm optimization
Phase 1: Coarse grid over parameters and architecture
Phase 2: Fine architecture search per algorithm with optimal parameters

Overall Best Configuration:
  Algorithm: SGD
  Parameters: {'lr': 0.05, 'momentum': 0.99}
  Hidden Units: 28
  RMSE: 0.9547 ± 0.2095

Per-Algorithm Final Results:
  1. SGD: h=28, RMSE=0.9547±0.2095
      Parameters: {'lr': 0.05, 'momentum': 0.99}
      Convergence Epoch: 160.8
      Convergence Time: 0.042s
      Total Training Time: 0.04s
  2. LEAPFROG: h=34, RMSE=1.0349±0.1696
      Parameters: {'dt': 0.001, 'delta_max': 1.0, 'xi': 0.1, 'm': 5}
      Convergence Epoch: 102.7
      Convergence Time: 0.050s
      Total Training Time: 0.05s
  3. SCG: h=32, RMSE=1.4127±0.2555
      Parameters: {'sigma': 1e-05, 'lambd': 0.01}
      Convergence Epoch: 128.7
      Convergence Time: 0.090s
      Total Training Time: 0.18s

Phase 1 Summary (180 configurations tested):
   1. LEAPFROG h=30 RMSE=0.9265±0.1447
   2. LEAPFROG h=10 RMSE=0.9757±0.2550
   3. LEAPFROG h=30 RMSE=1.0030±0.1868
   4. LEAPFROG h=15 RMSE=1.0037±0.2387
   5. LEAPFROG h=30 RMSE=1.0094±0.1197
   6. LEAPFROG h=10 RMSE=1.0196±0.2241
   7. LEAPFROG h=10 RMSE=1.0246±0.2886
   8. LEAPFROG h=15 RMSE=1.0302±0.1745
   9. LEAPFROG h=20 RMSE=1.0344±0.1610
  10. SGD h=30 RMSE=1.0347±0.1852
