JOINT HYPERPARAMETER AND ARCHITECTURE OPTIMIZATION - TITANIC
============================================================

Methodology: Two-phase per-algorithm optimization
Phase 1: Coarse grid over parameters and architecture
Phase 2: Fine architecture search per algorithm with optimal parameters

Overall Best Configuration:
  Algorithm: SGD
  Parameters: {'lr': 0.1, 'momentum': 0.0}
  Hidden Units: 22
  Accuracy: 0.8372 ± 0.0308

Per-Algorithm Final Results:
  1. SGD: h=22, Accuracy=0.8372±0.0308
      Parameters: {'lr': 0.1, 'momentum': 0.0}
      Convergence Epoch: 832.7
      Convergence Time: 0.237s
      Total Training Time: 0.27s
  2. SCG: h=16, Accuracy=0.8305±0.0274
      Parameters: {'sigma': 1e-05, 'lambd': 0.001}
      Total Training Time: 7.21s
  3. LEAPFROG: h=27, Accuracy=0.7968±0.0226
      Parameters: {'dt': 0.001, 'delta_max': 0.1, 'xi': 0.2, 'm': 5}
      Total Training Time: 92.61s

Phase 1 Summary (180 configurations tested):
   1. SGD h=20 Accuracy=0.8383±0.0249
   2. SGD h=15 Accuracy=0.8361±0.0332
   3. SGD h=25 Accuracy=0.8350±0.0368
   4. SGD h=20 Accuracy=0.8339±0.0276
   5. SGD h=10 Accuracy=0.8338±0.0276
   6. SGD h=15 Accuracy=0.8338±0.0374
   7. SCG h=20 Accuracy=0.8338±0.0272
   8. SGD h=30 Accuracy=0.8316±0.0346
   9. SGD h=25 Accuracy=0.8316±0.0259
  10. SGD h=10 Accuracy=0.8316±0.0259
